---
id: use-gpus
title: Using GPUs in Nebari projects
description: Provide a complete overview of using GPUs in the Nebari project, including server setup, environment setup, and validation.
---

# Using GPUs
## Introduction
This document provides a complete overview of using GPUs in the Nebari project, including server setup, environment setup, and validation.

## 1. Starting a GPU server

Follow steps 1 to 3 in the [tutorials/Authenticate and launch JupyterLab][login-with-keycloak] document. UI will show a list of profiles (instances, servers, machines).

![Nebari select profile](/img/how-tos/nebari_select_profile.png)
Your administrator pre-configures these options, as described in [Profile Configuration documentation][profile-configuration].

Select the server option most suitable for your project. If the server instance description does not explicitly mention GPU, this document has a section to validate your setup for GPUs.
Select an appropriate profile and click "Start".

### Understanding GPU setup on the server.
Before we create an environment, we need to check a few things on the server.
The following steps describe how to get CUDA-related information from the server.
1. Once the JupyterHub starts, it will redirect you to a JupyterLab home page.
2. Click on the **"Terminal"** icon.
3. Run the command `nvidia-smi`. The top right corner of the command's output should have the highest supported driver.
    ![nvidia-smi-output](/img/how-tos/nvidia-smi-output.png)

    If you get `nvidia-smi: command not found`. You are most likely on a non-GPU server. Contact the Nebari administrator to figure out the right instance option.

## 2. Creating environments

### Building a GPU-compatibly conda-store environment.
By default, conda-store will build CPU-compatible packages. To build GPU-compatible packages, we have a couple of options:
1. Use a GPU-specific version of PyTorch or TensorFlow, i.e., `pytorch-gpu` or `tensorflow-gpu`.
2. Build a GPU-enabled environment using `CONDA_OVERRIDE_CUDA`.
    1. You will need to check PyTorch documentation to [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/) to determine the latest override version for `CONDA_OVERRIDE_CUDA`.
    Select the following options to get the latest CUDA version:
    - PyTorch Build = Stable
    - Your OS = Linux
    - Package = Conda
    - Language = Python

    ![pytorch-linux-conda-version](/img/how-tos/pytorch-linux-conda-version.png)

    :::note
    - At the time of writing this document, the latest CUDA version was showing as `12.1`. Please follow the above step to determine the latest override value for the `CONDA_OVERRIDE_CUDA` environment variable.
    - Please ensure that your choice from PyTorch documentation is not greater than the highest supported version in the `nvidia-smi` output (captured above).
    :::
    2. Conda-store provides a mechanism to enable GPU environments via the setting of an environment variable as explained in the [conda-store docs](https://conda.store/conda-store-ui/tutorials/create-envs#set-environment-variables).
    3. To set the environment variable `CONDA_OVERRIDE_CUDA` in conda-store, click on the **GUI <-> YAML** Toggle to expose the underlying config. You will see a config ending with `variables: {}`. We will replace this section with an override for `CONDA_OVERRIDE_CUDA` to tell the conda-store to build a GPU-compatible environment instead of the default CPU-compatible one.
    ![conda-store-yaml-toggle](/img/how-tos/conda-store-yaml-toggle.png)
    ![pytorch-cuda-override.png](/img/how-tos/pytorch-cuda-override.png)

### Launching a new notebook with GPU-compatible environment
Go to the JupyterLab Homepage and find the newly created environment for GPUs under the "Launch New Notebook" section.
Click on the env to launch a JupyterNotebook.

## 3. Validating the setup
Run the following commands to get more information on the setup:
```
import torch
print(f"GPU available: {torch.cuda.is_available()}")
print(f"Number of GPUs available: {torch.cuda.device_count()}")
print(f"ID of current GPU: {torch.cuda.current_device()}")
print(f"Name of first GPU: {torch.cuda.get_device_name(0)}")
```
Your output should look something like this:

![jupyter-notebook-command-output](/img/how-tos/pytorch-cuda-check.png)

## Related links:
- [PyTorch best practices][pytorch-best-practices]

<!-- Internal links -->
[profile-configuration]: /docs/explanations/profile-configuration
[login-with-keycloak]: /docs/tutorials/login-keycloak
[pytorch-best-practices]: /docs/how-tos/pytorch-best-practices
